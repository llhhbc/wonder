<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on miracle of light</title>
    <link>http://wonder.zxcsoft.com/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on miracle of light</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Thu, 23 Aug 2018 19:01:00 +0800</lastBuildDate>
    
	<atom:link href="http://wonder.zxcsoft.com/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>kbuernetes docs 学习</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kuber_docs/</link>
      <pubDate>Thu, 23 Aug 2018 19:01:00 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kuber_docs/</guid>
      <description>kubernetes docs 学习  参考 http://docs.kubernetes.org.cn/251.html 对应源地址 https://kubernetes.feisky.xyz/zh/  kubernetes架构  etcd 保存整个集群的状态 apiserver 提供资源操作的唯一入口，并提供认证、授权、访问控制、api注册和发现等机制 controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等 scheduler负责资源的调度，按照预定的调度策略将pod调度到相应的机器上 kubelet 负责维护容器的生命周期，同时也负责volume（cvi）和网络（cni）的管理 container runtime负责镜像管理以及pod和容器的真正运行（cri） kube-proxy 负责为service提供cluster内部的服务发现和负载均衡  除了这些核心组件，还有些扩展的add-ons： * kube-dns负责为整个集群提供dns服务 * ingress controller为服务提供外网入口 * heapster提供资源监控 * dashboard提供gui * fedreation提供跨可用区的集群 * fluentd-elasticsearch提供集群日志采集、存储与查询
kuber分层架构  核心层：kubernetes最核心的功能，对外提供api构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、dns解析等） 管理层：系统度量（如基础设置、容器和网络的度量），自动化（如自动扩展、动态Provision等）以及策略管理（RBAC，quota，psp，networkpolicy等） 接口层：kubectl命令行工具、客户端sdk以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分两个  kubernetes外部：日志、监控、配置管理、ci,cd,workflow,faas,ots应用,chatops kubernetes内部：cri,cni,cvi,镜像仓库，cloud provider,集群自身的配置和管理等   kubernetse的设计理念 api设计原则  所有api应该是声明式的（比如设置副本数为3，运行多次也没问题，而给副本数加1，运行多次就不对了） api对象是彼此互补而且可以组合的 高层api以操作意图为基础设计（从业务出发，而不是过早的从技术实现出发） 低层api根据高层api的控制需要设计（以需求为基础，尽量抵抗受技术实现影响的诱惑） 尽量避免简单封装，不要有在外部api无法显式知道的内部隐藏的机制（简单的封装实际没有提供新功能，反而增加对所封装api的依赖性。内部隐藏的机制不利于系统维护的设计方式） api操作复杂试与对象数量成正比 api对象状态不能依赖于网络连接状态（对象状态能应对网络不稳定） 尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难  控制机制设计原则  控制逻辑应该只依赖于当前状态 假设任何错误的可能，并做差错处理 尽量避免复杂状态机，控制逻辑不要依赖无法监控的内部状态 假设任何操作都可能被任何操作对象拒绝，甚至被错误解析（保证出现错误的时候，操作级别的错误不会影响到系统稳定性） 每个模块都可以在出错后自动恢复 每个模块都可以在必要时优雅的降级服务（划分清楚基本功能和高级功能，保证基本功能不会依赖高级功能，这样就保证了不会因为高级功能出现故障而导致整个模块崩溃）  kuber api对象  pod pod内的容器共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。比如，一个nginx容器用来发布软件，另一个容器专门用来从源仓库做同步 pod是所有业务类型的基础，k8s中业务主要分四类：  Deployment：长期伺服型（long-runing） Job：批处理型（batch） DaemonSet：节点后台支撑型（node-daemon） PetSet：有状态应用型（stateful application）    pod特征：  共享ipc、network、utc namespace的容器 所有pod内容器都可以访问共享的Volume，可以访问共享数据 pod一旦调度后就和node绑定，即使node挂掉也不会重新调度，推荐使用deployments、daemonsets等控制器来容错 优雅终止：pod删除的时候，先给其内的进程发送sigterm，等待一段时间（graceperiod）后才强制停止依然还在运行的进程 特权容器（通过SecurityContext配置）具有改变系统配置的权限（在网络插件中大量应用）    复制控制器（Replication Controller，RC） RC是k8s集群中最早的保证pod高可用的api对象，监控运行中的pod来保证集群中运行指定数目的pod副本。只适用于长期伺服型的业务类型</description>
    </item>
    
    <item>
      <title>fluentd使用说明</title>
      <link>http://wonder.zxcsoft.com/post/kuber/fluentd/</link>
      <pubDate>Sun, 05 Aug 2018 10:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/fluentd/</guid>
      <description>fluentd配置说明  参考：https://docs.fluentd.org/v1.0/articles/filter-plugin-overview  filter配置说明  filter根据tag来匹配，可有多个filter对应同一个tag，处理按配置的顺序依次处理  筛选消息中包括cool的
&amp;lt;filter foo.bar&amp;gt; @type grep regexp1 message cool &amp;lt;/filter&amp;gt;  目前自带的filter有4个  filter_record_transformer 报文转换  语法格式：
&amp;lt;record&amp;gt; NEW_FIELD NEW_VALUE &amp;lt;/record&amp;gt;  tag的处理方法：
//tag_parts[N] 表示tag的第N个部分 tag_prefix[0] = debug tag_suffix[0] = debug.my.app tag_prefix[1] = debug.my tag_suffix[1] = my.app tag_prefix[2] = debug.my.app tag_suffix[2] = app  &amp;lt;filter foo.bar&amp;gt; @type record_transformer &amp;lt;record&amp;gt; hostname &amp;quot;#{Socket.gethostname}&amp;quot; #给报文增加两个域：hostname，tag tag ${tag} avg ${record[&amp;quot;total&amp;quot;] / record[&amp;quot;count&amp;quot;]} ##增加一个avg域，计算报文中total和count的除数 message yay, ${record[&amp;quot;message&amp;quot;]} ## 给message域增加个前缀 &amp;lt;/record&amp;gt; &amp;lt;/filter&amp;gt;   filter_grep 报文筛选  &amp;lt;filter foo.</description>
    </item>
    
    <item>
      <title>kuber log analysis</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kuber-logs/</link>
      <pubDate>Sun, 05 Aug 2018 10:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kuber-logs/</guid>
      <description>kubernetes日志管理 参考资料：https://logz.io/blog/kubernetes-log-analysis/
1. 安装fluentd  参考官方文档：https://docs.fluentd.org/v1.0/articles/install-by-rpm#redhat-/-centos
 td-agent是对fluentd的封装，并加入了管理工具，比如java有集成的应用包
  curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh | sh #启动td-agent systemctl start td-agent #安装td-agent es插件 sudo /usr/sbin/td-agent-gem install fluent-plugin-elasticsearch --no-document  2. 配置说明  配置td-agent： 备份 /etc/td-agent/td-agent.conf  ## 增加一个测试的日志输入，用于测试解析日志并写入es &amp;lt;source&amp;gt; @type http @id input_http port 42185 tag http.test &amp;lt;/source&amp;gt; # 配置将收到的日志输入到控制台 &amp;lt;match **&amp;gt; @type stdout @id output_stdout &amp;lt;/match&amp;gt; # 配置将日志写入es &amp;lt;match http.test&amp;gt; #一个source的tag只会匹配一个match，有多个时，不会都匹配 @type elasticsearch logstash_format true host localhost port 9200 #hosts host1:port1,host2:port2,host3:port3 # or #hosts https://customhost.</description>
    </item>
    
    <item>
      <title>kuber应用搭建</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kuber_myapp/</link>
      <pubDate>Sun, 29 Jul 2018 16:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kuber_myapp/</guid>
      <description>dreammap搭建 1. 应用打包docker容器 1.1 前端界面  dockerfile  from nginx:1.15.2-alpine COPY wonder-dream /usr/share/nginx/html EXPOSE 80   build.sh  function build(){ cp -rp ../dist/wonder-dream . docker rmi wonderdream:v0.0.1 docker build -t wonderdream:v0.0.1 . } function run(){ docker run -itd -p 8091:80 wonderdream:v0.0.1 } function tar(){ docker save -o wonder_ng.tgz wonderdream:v0.0.1 } $1   kuber部署yaml，这里就用service实现了访问，没有做单独的ingress  apiVersion: v1 kind: Service metadata: name: wonderng labels: app: wonderng spec: type: LoadBalancer ports: - port: 80 name: http selector: app: wonderng --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: wonderdream-v0.</description>
    </item>
    
    <item>
      <title>traefik服务搭建</title>
      <link>http://wonder.zxcsoft.com/post/kuber/traefik/</link>
      <pubDate>Sun, 29 Jul 2018 16:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/traefik/</guid>
      <description>Traefik服务使用 尽管svc有了负载均衡功能，可以简单通过LoadBalance来实现，但功能相对简单，而且有多个服务的时候，不好统一管理，而traefik是一个反向代理，可以像nginx一样配置相应的服务代理功能，并增加了检查服务是否可用、pod状态等功能，动态的更新配置
1.Traefik的部署  参考官方文档：https://docs.traefik.io/user-guide/kubernetes/
  创建角色：因为traefik需要访问kuber来获取服务等的状态信息  kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller rules: - apiGroups: - &amp;quot;&amp;quot; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: default   创建服务 traefik本身也是一种服务，和其它服务一样，80是工作端口（服务分发），8080是ui端口，可查看当前情况  apiVersion: v1 kind: ServiceAccount metadata: name: traefik-ingress-controller namespace: default --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: traefik-ingress-controller namespace: default labels: k8s-app: traefik-ingress-lb spec: replicas: 1 selector: matchLabels: k8s-app: traefik-ingress-lb template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 containers: - image: traefik:v1.</description>
    </item>
    
    <item>
      <title>kuber单机部署</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kuber_sign/</link>
      <pubDate>Wed, 25 Jul 2018 20:00:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kuber_sign/</guid>
      <description>kuber单机部署 由于在阿里上只有一台，又不准备用minikube，所以单机部署一个
1.安装docker wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-18.06.0.ce-3.el7.x86_64.rpm yum install docker-ce-18.06.0.ce-3.el7.x86_64.rpm  2.安装etcd  生成根证书  { &amp;quot;key&amp;quot;: { &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;, &amp;quot;size&amp;quot;: 4096 }, &amp;quot;names&amp;quot;: [ { &amp;quot;O&amp;quot;: &amp;quot;wonder&amp;quot;, &amp;quot;OU&amp;quot;: &amp;quot;wonder Security&amp;quot;, &amp;quot;L&amp;quot;: &amp;quot;Sh&amp;quot;, &amp;quot;ST&amp;quot;: &amp;quot;Sh&amp;quot;, &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot; } ], &amp;quot;CN&amp;quot;: &amp;quot;wonder-root-ca&amp;quot; }  3.自动化脚本配置如下 #!/bin/zsh . ./config/env function set_path(){ for IP in $MASTER;do ssh root@$IP &#39;echo &amp;quot;export PATH=\$PATH:/opt/kubernetes/bin/&amp;quot; &amp;gt;&amp;gt; ~/.bash_profile&#39; done } function set_hosts(){ ##set hosts for IP in $MASTER $NODE;do scp .</description>
    </item>
    
    <item>
      <title>kbuernetes 学习</title>
      <link>http://wonder.zxcsoft.com/post/kuber/learn_kuber/</link>
      <pubDate>Fri, 27 Apr 2018 09:01:00 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/learn_kuber/</guid>
      <description>Kubernetes基本概念 Pod Pod是一组容器集合，他们共享IPC、Network 和 UTC namespace 例：
apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80  Node 运行pod的主机
Namespace 管理一组资源和对象
Service 应用服务的抽象，通过labels为应用提供负载均衡和服务发现。匹配labels为Pod IP和端口列表组成endpoints，由kube-proxy负责将服务IP负载均衡到这些endpoints上。 每个Service都会自动分配一个culster IP（仅在集群内部可访问的虚拟地址）和DNS名，其它容器可以通过该地址或DNS来访问服务
apiVersion: v1 kind: Service metadata: name: nginx spec: ports: - port: 8078 name: http targetPort: 80 protocol: TCP selector: app: nginx  Label 是识别Kurbernetes对象的标签，以key/value的方式附加到对象上。Label不提供唯一性，经常是很多对象（如Pods）都使用相同的label来标志具体的应用（如负载均衡时结点为的选择） label选择支持如下模式： * 等式： app=nginx 或 env!= production * 集合: env in (production, qa) * 多个label（他们之间是AND的关系）： app=nginx,env=test</description>
    </item>
    
    <item>
      <title>istio初尝试</title>
      <link>http://wonder.zxcsoft.com/post/kuber/istio/</link>
      <pubDate>Thu, 26 Apr 2018 20:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/istio/</guid>
      <description>说明  参考 http://istio.doczh.cn/
 安装istio  kubectl apply -f install/kubernetes/istio.yaml #如果出现 unable to recognize &amp;ldquo;install/kubernetes/istio.yaml&amp;rdquo; 的错误，删除后再重新执行一遍就好了
 部署bookinfo kubectl apply -f &amp;lt;(istioctl kube-inject -f bookinfo.yaml) #获取访问地址 export GATEWAY_URL=$(kubectl get po -l istio=ingress -n istio-system -o &#39;jsonpath={.items[0].status.hostIP}&#39;):$(kubectl get svc istio-ingress -n istio-system -o &#39;jsonpath={.spec.ports[0].nodePort}&#39;) #测试地址访问 curl -o /dev/null -s -w &amp;quot;%{http_code}\n&amp;quot; http://${GATEWAY_URL}/productpage  1.1 验证路由访问 #所有用户都访问v1 istioctl create -f route-rule-all-v1.yaml #jason用户登录访问v2 istioctl create -f route-rule-reviews-test-v2.yaml  1.2 记录日志 ##保存如下信息为 new_telemetry.</description>
    </item>
    
    <item>
      <title>kuber手工搭建</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kuber_onebyone/</link>
      <pubDate>Thu, 26 Apr 2018 20:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kuber_onebyone/</guid>
      <description>准备工作 关闭selunix 关闭swap 作为node的结点，要安装docker（包括master和node共用的）
wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-18.06.0.ce-3.el7.x86_64.rpm yum install docker-ce-18.06.0.ce-3.el7.x86_64.rpm  设置主机名
 sh etc/set_hosts.sh
 安装etcd
 sh rpms/install_etcd.sh
 安装etcd证书
 sh keys/etcd/install_etc_key.sh
 修改etcd配置
 sh etc/set_etcd.sh
 启动etcd
 sh run/start_etcd.sh
 安装kube 程序
 sh rpms/install_kuber.sh
 安装kuber证书
 sh keys/k8s/install_k8s_key.sh
 修改kuber配置
 #设置master结点
sh etc/kube_cfg/set_kuber_cfg.sh
#设置node结点
sh etc/kube_cfg/set_kuber_node_cfg.sh
#如果master也作为node结点，则运行
sh etc/kube_cfg/set_kuber_master_node_cfg.sh
 启动kube
 sh run/start_kube.sh
#在任意一台master上执行： 开启认证
kubectl create clusterrolebinding kubelet-bootstrap &amp;ndash;clusterrole=system:node-bootstrapper &amp;ndash;user=kubelet-bootstrap</description>
    </item>
    
    <item>
      <title>kuberadm start</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kubeadm_start/</link>
      <pubDate>Thu, 26 Apr 2018 10:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kubeadm_start/</guid>
      <description>kubernetes搭建比较复杂，所以选择先用kubeadm全自动来先试试手（奈何墙有点高，所以加了些插曲）
通过kubeadm安装 参考 官方文档
1. 准备环境 1.1 修改hostname  修改 /etc/hostname  1.2 关闭sellinux，关闭防火墙  setenforce 0 编辑/etc/selinux/config
 firewall-cmd &amp;ndash;state 查看状态
 systemctl stop firewalld.service # 停止firewall
 systemctl disable firewalld.service # 禁止firewall开机启动
  1.3 关闭swap swapoff -a 编辑 /etc/fstab 去掉swap配置（#号注释掉）  1.4 安装crictl (可选) go get github.com/kubernetes-incubator/cri-tools/cmd/crictl GOARCH=amd64 GOOS=linux go build  1.5 get docker image  运行kubeadm init 当提示请稍等后，检查/etc/kubernetes/manifests 目录下的yaml文件，里面会有需要的镜像和版本 通过hub.docker.com 中转，实现镜像的下载 具体方法请参考：kubeadm搭建（by mritd） 重新tag镜像(以下是我使用的，可直接pull后使用)</description>
    </item>
    
    <item>
      <title>kube问题汇总</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kube_help/</link>
      <pubDate>Thu, 26 Apr 2018 10:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kube_help/</guid>
      <description> 虚拟机建议用virutalbox
当时istio的bookinfo无法部署： reviews-v2-7bdf9b96b6-khg7s 老是会报错，提示无法创建目录，没权限，后来查是虚拟机sandbox版本太低，导致的一个bug，文件无法删除也无法修改
 virtualbox建议用nat 网络的方式，自己添加一个网卡，作为虚拟机集群的网络
然后每个主机在加一个hostonly的网卡，用于主机访问虚拟机
 硬盘没空间：
突然发现pod状态变成了：Evicted，还有挂起的，然后 通过 kubelet describe命令查看，发现是node空间满了，无法部署了：我看空间用了80%
 时间不同步：
时间不同步时，会出现 Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid 我同步机器时间后，问题解决。。。
 token不一致：
配置文件bootstrap.kubeconfig中token不一致，会导致这个错 failed to run Kubelet: cannot create certificate signing request: Unauthorized
  </description>
    </item>
    
  </channel>
</rss>