<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on miracle of light</title>
    <link>http://wonder.zxcsoft.com/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on miracle of light</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Sun, 05 Aug 2018 10:46:49 +0800</lastBuildDate>
    
	<atom:link href="http://wonder.zxcsoft.com/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>kuber log analysis</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kuber-logs/</link>
      <pubDate>Sun, 05 Aug 2018 10:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kuber-logs/</guid>
      <description>kubernetes日志管理 参考资料：https://logz.io/blog/kubernetes-log-analysis/
1. 安装fluentd  参考官方文档：https://docs.fluentd.org/v1.0/articles/install-by-rpm#redhat-/-centos
 td-agent是对fluentd的封装，并加入了管理工具，比如java有集成的应用包
  curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh | sh #启动td-agent systemctl start td-agent #安装td-agent es插件 sudo /usr/sbin/td-agent-gem install fluent-plugin-elasticsearch --no-document  2. 配置说明  配置td-agent： 备份 /etc/td-agent/td-agent.conf  ## 增加一个测试的日志输入，用于测试解析日志并写入es &amp;lt;source&amp;gt; @type http @id input_http port 42185 tag http.test &amp;lt;/source&amp;gt; # 配置将收到的日志输入到控制台 &amp;lt;match **&amp;gt; @type stdout @id output_stdout &amp;lt;/match&amp;gt; # 配置将日志写入es &amp;lt;match http.test&amp;gt; @type elasticsearch logstash_format true host localhost port 9200 #hosts host1:port1,host2:port2,host3:port3 # or #hosts https://customhost.</description>
    </item>
    
    <item>
      <title>kuber应用搭建</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kuber_myapp/</link>
      <pubDate>Sun, 29 Jul 2018 16:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kuber_myapp/</guid>
      <description>dreammap搭建 1. 应用打包docker容器 1.1 前端界面  dockerfile  from nginx:1.15.2-alpine COPY wonder-dream /usr/share/nginx/html EXPOSE 80   build.sh  function build(){ cp -rp ../dist/wonder-dream . docker rmi wonderdream:v0.0.1 docker build -t wonderdream:v0.0.1 . } function run(){ docker run -itd -p 8091:80 wonderdream:v0.0.1 } function tar(){ docker save -o wonder_ng.tgz wonderdream:v0.0.1 } $1   kuber部署yaml，这里就用service实现了访问，没有做单独的ingress  apiVersion: v1 kind: Service metadata: name: wonderng labels: app: wonderng spec: type: LoadBalancer ports: - port: 80 name: http selector: app: wonderng --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: wonderdream-v0.</description>
    </item>
    
    <item>
      <title>traefik服务搭建</title>
      <link>http://wonder.zxcsoft.com/post/kuber/traefik/</link>
      <pubDate>Sun, 29 Jul 2018 16:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/traefik/</guid>
      <description>Traefik服务使用 尽管svc有了负载均衡功能，可以简单通过LoadBalance来实现，但功能相对简单，而且有多个服务的时候，不好统一管理，而traefik是一个反向代理，可以像nginx一样配置相应的服务代理功能，并增加了检查服务是否可用、pod状态等功能，动态的更新配置
1.Traefik的部署  参考官方文档：https://docs.traefik.io/user-guide/kubernetes/
  创建角色：因为traefik需要访问kuber来获取服务等的状态信息  kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller rules: - apiGroups: - &amp;quot;&amp;quot; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: default   创建服务 traefik本身也是一种服务，和其它服务一样，80是工作端口（服务分发），8080是ui端口，可查看当前情况  apiVersion: v1 kind: ServiceAccount metadata: name: traefik-ingress-controller namespace: default --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: traefik-ingress-controller namespace: default labels: k8s-app: traefik-ingress-lb spec: replicas: 1 selector: matchLabels: k8s-app: traefik-ingress-lb template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 containers: - image: traefik:v1.</description>
    </item>
    
    <item>
      <title>kuber单机部署</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kuber_sign/</link>
      <pubDate>Wed, 25 Jul 2018 20:00:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kuber_sign/</guid>
      <description>kuber单机部署 由于在阿里上只有一台，又不准备用minikube，所以单机部署一个
1.安装docker wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-18.06.0.ce-3.el7.x86_64.rpm yum install docker-ce-18.06.0.ce-3.el7.x86_64.rpm  2.安装etcd  生成根证书  { &amp;quot;key&amp;quot;: { &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;, &amp;quot;size&amp;quot;: 4096 }, &amp;quot;names&amp;quot;: [ { &amp;quot;O&amp;quot;: &amp;quot;wonder&amp;quot;, &amp;quot;OU&amp;quot;: &amp;quot;wonder Security&amp;quot;, &amp;quot;L&amp;quot;: &amp;quot;Sh&amp;quot;, &amp;quot;ST&amp;quot;: &amp;quot;Sh&amp;quot;, &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot; } ], &amp;quot;CN&amp;quot;: &amp;quot;wonder-root-ca&amp;quot; }  3.自动化脚本配置如下 #!/bin/zsh . ./config/env function set_path(){ for IP in $MASTER;do ssh root@$IP &#39;echo &amp;quot;export PATH=\$PATH:/opt/kubernetes/bin/&amp;quot; &amp;gt;&amp;gt; ~/.bash_profile&#39; done } function set_hosts(){ ##set hosts for IP in $MASTER $NODE;do scp .</description>
    </item>
    
    <item>
      <title>kbuernetes 学习</title>
      <link>http://wonder.zxcsoft.com/post/kuber/learn_kuber/</link>
      <pubDate>Fri, 27 Apr 2018 09:01:00 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/learn_kuber/</guid>
      <description>Kubernetes基本概念 Pod Pod是一组容器集合，他们共享IPC、Network 和 UTC namespace 例：
apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80  Node 运行pod的主机
Namespace 管理一组资源和对象
Service 应用服务的抽象，通过labels为应用提供负载均衡和服务发现。匹配labels为Pod IP和端口列表组成endpoints，由kube-proxy负责将服务IP负载均衡到这些endpoints上。 每个Service都会自动分配一个culster IP（仅在集群内部可访问的虚拟地址）和DNS名，其它容器可以通过该地址或DNS来访问服务
apiVersion: v1 kind: Service metadata: name: nginx spec: ports: - port: 8078 name: http targetPort: 80 protocol: TCP selector: app: nginx  Label 是识别Kurbernetes对象的标签，以key/value的方式附加到对象上。Label不提供唯一性，经常是很多对象（如Pods）都使用相同的label来标志具体的应用（如负载均衡时结点为的选择） label选择支持如下模式： * 等式： app=nginx 或 env!= production * 集合: env in (production, qa) * 多个label（他们之间是AND的关系）： app=nginx,env=test</description>
    </item>
    
    <item>
      <title>istio初尝试</title>
      <link>http://wonder.zxcsoft.com/post/kuber/istio/</link>
      <pubDate>Thu, 26 Apr 2018 20:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/istio/</guid>
      <description>说明  参考 http://istio.doczh.cn/
 安装istio  kubectl apply -f install/kubernetes/istio.yaml #如果出现 unable to recognize &amp;ldquo;install/kubernetes/istio.yaml&amp;rdquo; 的错误，删除后再重新执行一遍就好了
 部署bookinfo kubectl apply -f &amp;lt;(istioctl kube-inject -f bookinfo.yaml) #获取访问地址 export GATEWAY_URL=$(kubectl get po -l istio=ingress -n istio-system -o &#39;jsonpath={.items[0].status.hostIP}&#39;):$(kubectl get svc istio-ingress -n istio-system -o &#39;jsonpath={.spec.ports[0].nodePort}&#39;) #测试地址访问 curl -o /dev/null -s -w &amp;quot;%{http_code}\n&amp;quot; http://${GATEWAY_URL}/productpage  1.1 验证路由访问 #所有用户都访问v1 istioctl create -f route-rule-all-v1.yaml #jason用户登录访问v2 istioctl create -f route-rule-reviews-test-v2.yaml  1.2 记录日志 ##保存如下信息为 new_telemetry.</description>
    </item>
    
    <item>
      <title>kuber手工搭建</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kuber_onebyone/</link>
      <pubDate>Thu, 26 Apr 2018 20:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kuber_onebyone/</guid>
      <description>准备工作 关闭selunix 关闭swap 作为node的结点，要安装docker（包括master和node共用的）
wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-18.06.0.ce-3.el7.x86_64.rpm yum install docker-ce-18.06.0.ce-3.el7.x86_64.rpm  设置主机名
 sh etc/set_hosts.sh
 安装etcd
 sh rpms/install_etcd.sh
 安装etcd证书
 sh keys/etcd/install_etc_key.sh
 修改etcd配置
 sh etc/set_etcd.sh
 启动etcd
 sh run/start_etcd.sh
 安装kube 程序
 sh rpms/install_kuber.sh
 安装kuber证书
 sh keys/k8s/install_k8s_key.sh
 修改kuber配置
 #设置master结点
sh etc/kube_cfg/set_kuber_cfg.sh
#设置node结点
sh etc/kube_cfg/set_kuber_node_cfg.sh
#如果master也作为node结点，则运行
sh etc/kube_cfg/set_kuber_master_node_cfg.sh
 启动kube
 sh run/start_kube.sh
#在任意一台master上执行： 开启认证
kubectl create clusterrolebinding kubelet-bootstrap &amp;ndash;clusterrole=system:node-bootstrapper &amp;ndash;user=kubelet-bootstrap</description>
    </item>
    
    <item>
      <title>kuberadm start</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kubeadm_start/</link>
      <pubDate>Thu, 26 Apr 2018 10:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kubeadm_start/</guid>
      <description>kubernetes搭建比较复杂，所以选择先用kubeadm全自动来先试试手（奈何墙有点高，所以加了些插曲）
通过kubeadm安装 参考 官方文档
1. 准备环境 1.1 修改hostname  修改 /etc/hostname  1.2 关闭sellinux，关闭防火墙  setenforce 0 编辑/etc/selinux/config
 firewall-cmd &amp;ndash;state 查看状态
 systemctl stop firewalld.service # 停止firewall
 systemctl disable firewalld.service # 禁止firewall开机启动
  1.3 关闭swap swapoff -a 编辑 /etc/fstab 去掉swap配置（#号注释掉）  1.4 安装crictl (可选) go get github.com/kubernetes-incubator/cri-tools/cmd/crictl GOARCH=amd64 GOOS=linux go build  1.5 get docker image  运行kubeadm init 当提示请稍等后，检查/etc/kubernetes/manifests 目录下的yaml文件，里面会有需要的镜像和版本 通过hub.docker.com 中转，实现镜像的下载 具体方法请参考：kubeadm搭建（by mritd） 重新tag镜像(以下是我使用的，可直接pull后使用)</description>
    </item>
    
    <item>
      <title>kube问题汇总</title>
      <link>http://wonder.zxcsoft.com/post/kuber/kube_help/</link>
      <pubDate>Thu, 26 Apr 2018 10:46:49 +0800</pubDate>
      
      <guid>http://wonder.zxcsoft.com/post/kuber/kube_help/</guid>
      <description> 虚拟机建议用virutalbox
当时istio的bookinfo无法部署： reviews-v2-7bdf9b96b6-khg7s 老是会报错，提示无法创建目录，没权限，后来查是虚拟机sandbox版本太低，导致的一个bug，文件无法删除也无法修改
 virtualbox建议用nat 网络的方式，自己添加一个网卡，作为虚拟机集群的网络
然后每个主机在加一个hostonly的网卡，用于主机访问虚拟机
 硬盘没空间：
突然发现pod状态变成了：Evicted，还有挂起的，然后 通过 kubelet describe命令查看，发现是node空间满了，无法部署了：我看空间用了80%
 时间不同步：
时间不同步时，会出现 Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid 我同步机器时间后，问题解决。。。
 token不一致：
配置文件bootstrap.kubeconfig中token不一致，会导致这个错 failed to run Kubelet: cannot create certificate signing request: Unauthorized
  </description>
    </item>
    
  </channel>
</rss>